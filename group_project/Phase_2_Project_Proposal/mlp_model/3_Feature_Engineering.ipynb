{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d8229da",
   "metadata": {},
   "source": [
    "IMPORTANT!! Remember to ONLY engineer based on training data. \n",
    "All engineering must be able to pass to testing and validation data \n",
    "WITHOUT needing to be directly viewed / manually analysed.\n",
    "\n",
    "Feature Engineering/Extraction\n",
    "\n",
    "Objective: \n",
    "(depends on output from Code_Inspect)\n",
    "If decision is to use Ling and Enron:\n",
    "- extract sender (if possible)\n",
    "- extract urls (if possible)            <-- PRIORITY --#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ccf5fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import URLExtraction\n",
    "import URLHandling\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ead3895",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prep import lists\n",
    "from_save = ['Nigerian_Fraud', 'Nazario', 'SpamAssasin', 'CEAS_08', 'Enron', 'Ling']\n",
    "save_levels = ['_train', '_val', '_test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0453997",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define list of dataframes\n",
    "df_train_list = []\n",
    "df_val_list = []\n",
    "df_test_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6fa6899e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define necessary class instances\n",
    "extract_urls = URLExtraction.url_extraction()\n",
    "replace_urls = URLHandling.url_handling()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "48fb184d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make output directory if it doesn't exist\n",
    "input_dir = Path(\"Split_Data/Uncleaned\")\n",
    "input_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for dataset in from_save:\n",
    "    for level in save_levels:\n",
    "        var_name = f\"{dataset}{level}\"\n",
    "        input_path = input_dir / f\"{var_name}.csv\"\n",
    "        df = pd.read_csv(input_path)\n",
    "        if df is not None:\n",
    "            '''\n",
    "            Dropping all columns other than subject, body, and label.\n",
    "                This way, we can use all 6 datasets\n",
    "                The url extractor designed in RULExtraction will be used to \n",
    "                    handle URLs. We will have count and distinct_count, which \n",
    "                    will contain all information (and more) from urls\n",
    "                        (urls was just a 0,1 column stating whether or \n",
    "                        not a url appeared in body)\n",
    "            '''\n",
    "            df0 = df.drop(columns=['sender', 'receiver', 'date', 'urls',], errors='ignore')\n",
    "            \n",
    "            # Fill null values in subject and body\n",
    "            df0[['subject', 'body']] = df0[['subject', 'body']].fillna('<missing>')\n",
    "\n",
    "            df1 = extract_urls.df_extractor(df0, 'body', 'url_dict')\n",
    "            df2 = replace_urls.url_replacement(df1, 'body', 'url_dict', 'cleaned_body', indexed=False)\n",
    "                # indexed=False by default: if we want <url1> <url2> ..., change to True\n",
    "            # Perform additional cleaning:\n",
    "                # extract url_count and distinct_url_count from url_dict\n",
    "            df2['url_count'] = df2['url_dict'].apply(len)\n",
    "            df2['distinct_url_count'] = df2['url_dict'].apply(lambda d: sum(d.values()))\n",
    "            df2['body'] = df2['cleaned_body']\n",
    "            df3 = df2.drop(columns=['cleaned_body', 'url_dict'])\n",
    "            \n",
    "            # # Fill null values in url counters\n",
    "            # df3[['subject', 'body']] = df3[['subject', 'body']].fillna('<missing>')\n",
    "            df3[['url_count', 'distinct_url_count']] = df3[['url_count', 'distinct_url_count']].fillna('<missing>')\n",
    "\n",
    "\n",
    "            df_final = df3 #placeholder: df2 is wrong: fix later.\n",
    "\n",
    "            # Append dataset to dataset list\n",
    "            if level == '_train':\n",
    "                df_train_list.append(df_final)\n",
    "            elif level == '_val':\n",
    "                df_val_list.append(df_final)\n",
    "            else:\n",
    "                df_test_list.append(df_final)\n",
    "        else:\n",
    "            print(f\"Dataset '{var_name}' not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f7e566c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # For testing: remove later\n",
    "# df = pd.DataFrame({'test': 'test'})\n",
    "# # End test: below is required. Merge with necessary loop\n",
    "# '''\n",
    "# Dropping all columns other than subject, body, and label.\n",
    "#     This way, we can use all 6 datasets\n",
    "#     The url extractor designed in RULExtraction will be used to \n",
    "#         handle URLs. We will have count and distinct_count, which \n",
    "#         will contain all information (and more) from urls\n",
    "#             (urls was just a 0,1 column stating whether or \n",
    "#             not a url appeared in body)\n",
    "# '''\n",
    "# df0 = df.drop(columns=['sender', 'receiver', 'date', 'urls',], errors='ignore')\n",
    "# df1 = extract_urls.df_extractor(df0, 'body', 'url_dict')\n",
    "# df2 = replace_urls.url_replacement(df1, 'body', 'url_dict', 'cleaned_body', indexed=False)\n",
    "#     # indexed=False by default: if we want <url1> <url2> ..., change to True\n",
    "# # Perform additional cleaning:\n",
    "#     # extract url_count and distinct_url_count from url_dict\n",
    "# df2['url_count'] = df2['url_dict'].apply(len)\n",
    "# df2['distinct_url_count'] = df2['url_dict'].apply(lambda d: sum(d.values()))\n",
    "# df2['body'] = df2['cleaned_body']\n",
    "# df3 = df2.drop(columns=['cleaned_body', 'url_dict'])\n",
    "\n",
    "# df_final = df3 #placeholder: df2 is wrong: fix later.\n",
    "\n",
    "# # Append dataset to dataset list\n",
    "# df_list.append(df_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "65b09389",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concat all dataframes\n",
    "df_train_combined = pd.concat(df_train_list, ignore_index=True)\n",
    "df_val_combined = pd.concat(df_val_list, ignore_index=True)\n",
    "df_test_combined = pd.concat(df_test_list, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fe7a8949",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dataset\n",
    "# Make output directory if it doesn't exist\n",
    "output_dir = Path(\"Split_Data/Cleaned\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "train_path = output_dir / f\"cleaned_train_data.csv\"\n",
    "val_path = output_dir / f\"cleaned_val_data.csv\"\n",
    "test_path = output_dir / f\"cleaned_test_data.csv\"\n",
    "\n",
    "df_train_combined.to_csv(train_path)\n",
    "df_val_combined.to_csv(val_path)\n",
    "df_test_combined.to_csv(test_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
